{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7v4DdOctd2U"
      },
      "source": [
        "## COMPARING DEEP Q-NETWORKS AND VAIRANTS ON ATARI."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install 'gym [atari,accept-rom-license]==0.21.0'"
      ],
      "metadata": {
        "id": "YFudUbzI6MZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NP11OuhMtd2V"
      },
      "source": [
        "## Please install the Pytorch library on your computer before you run this notebook.\n",
        "\n",
        "The installation instructions can be found here. (https://pytorch.org/get-started/locally/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKoYXIi3td2V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import tqdm\n",
        "import random\n",
        "import gym\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from gym.wrappers import AtariPreprocessing, FrameStack"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.version import VERSION\n",
        "print(VERSION)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROqRvaDGM-1q",
        "outputId": "aa0ef05f-fabd-4991-ee81-1e69e855a33c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Nw76V_7td2f"
      },
      "source": [
        "## Define a Deep Q network\n",
        "\n",
        "Before, we write a DQN agent. Let's define a Deep Q network as we did in Q1. Otherwise, you could also adapt your\n",
        "implementation above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXxMoETytd2f"
      },
      "outputs": [],
      "source": [
        "# customized weight initialization\n",
        "def customized_weights_init(m):\n",
        "    # compute the gain\n",
        "    gain = nn.init.calculate_gain('relu')\n",
        "    # init the convolutional layer\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        # init the params using uniform\n",
        "        nn.init.xavier_uniform_(m.weight, gain=gain)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "    # init the linear layer\n",
        "    if isinstance(m, nn.Linear):\n",
        "        # init the params using uniform\n",
        "        nn.init.xavier_uniform_(m.weight, gain=gain)\n",
        "        nn.init.constant_(m.bias, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um16hWTetd2f"
      },
      "source": [
        "## Define a Experience Replay Buffer\n",
        "\n",
        "One main contribution of DQN is proposing to use the replay buffer. Here is the implementation of a simple replay buffer as a list of transitions (i.e., [(s, a, r, s', d), ....]). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOCyMwCotd2f"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer(object):\n",
        "    \"\"\" Implement the Replay Buffer as a class, which contains:\n",
        "            - self._data_buffer (list): a list variable to store all transition tuples.\n",
        "            - add: a function to add new transition tuple into the buffer\n",
        "            - sample_batch: a function to sample a batch training data from the Replay Buffer\n",
        "    \"\"\"\n",
        "    def __init__(self, buffer_size):\n",
        "        \"\"\"Args:\n",
        "               buffer_size (int): size of the replay buffer\n",
        "        \"\"\"\n",
        "        # total size of the replay buffer\n",
        "        self.total_size = buffer_size\n",
        "\n",
        "        # create a list to store the transitions\n",
        "        self._data_buffer = []\n",
        "        self._next_idx = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._data_buffer)\n",
        "\n",
        "    def add(self, obs, act, reward, next_obs, done):\n",
        "        # create a tuple\n",
        "        trans = (obs, act, reward, next_obs, done)\n",
        "\n",
        "        # interesting implementation\n",
        "        if self._next_idx >= len(self._data_buffer):\n",
        "            self._data_buffer.append(trans)\n",
        "        else:\n",
        "            self._data_buffer[self._next_idx] = trans\n",
        "\n",
        "        # increase the index\n",
        "        self._next_idx = (self._next_idx + 1) % self.total_size\n",
        "\n",
        "    def _encode_sample(self, indices):\n",
        "        \"\"\" Function to fetch the state, action, reward, next state, and done arrays.\n",
        "        \n",
        "            Args:\n",
        "                indices (list): list contains the index of all sampled transition tuples.\n",
        "        \"\"\"\n",
        "        # lists for transitions\n",
        "        obs_list, actions_list, rewards_list, next_obs_list, dones_list = [], [], [], [], []\n",
        "\n",
        "        # collect the data\n",
        "        for idx in indices:\n",
        "            # get the single transition\n",
        "            data = self._data_buffer[idx]\n",
        "            obs, act, reward, next_obs, d = data\n",
        "            # store to the list\n",
        "            obs_list.append(np.array(obs, copy=False))\n",
        "            actions_list.append(np.array(act, copy=False))\n",
        "            rewards_list.append(np.array(reward, copy=False))\n",
        "            next_obs_list.append(np.array(next_obs, copy=False))\n",
        "            dones_list.append(np.array(d, copy=False))\n",
        "        # return the sampled batch data as numpy arrays\n",
        "        return np.array(obs_list), np.array(actions_list), np.array(rewards_list), np.array(next_obs_list), np.array(\n",
        "            dones_list)\n",
        "\n",
        "    def sample_batch(self, batch_size):\n",
        "        \"\"\" Args:\n",
        "                batch_size (int): size of the sampled batch data.\n",
        "        \"\"\"\n",
        "        # sample indices with replaced\n",
        "        indices = [np.random.randint(0, len(self._data_buffer)) for _ in range(batch_size)]\n",
        "        return self._encode_sample(indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEMjw-vttd2j"
      },
      "outputs": [],
      "source": [
        "def plot_curves(arr_list, legend_list, color_list, ylabel, fig_title):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        arr_list (list): list of results arrays to plot\n",
        "        legend_list (list): list of legends corresponding to each result array\n",
        "        color_list (list): list of color corresponding to each result array\n",
        "        ylabel (string): label of the Y axis\n",
        "\n",
        "        Note that, make sure the elements in the arr_list, legend_list and color_list are associated with each other correctly.\n",
        "        Do not forget to change the ylabel for different plots.\n",
        "    \"\"\"\n",
        "    # set the figure type\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # PLEASE NOTE: Change the labels for different plots\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.set_xlabel(\"Time Steps\")\n",
        "\n",
        "    # ploth results\n",
        "    h_list = []\n",
        "    for arr, legend, color in zip(arr_list, legend_list, color_list):\n",
        "        # compute the standard error\n",
        "        arr_err = arr.std(axis=0) / np.sqrt(arr.shape[0])\n",
        "        # plot the mean\n",
        "        h, = ax.plot(range(arr.shape[1]), arr.mean(axis=0), color=color, label=legend)\n",
        "        # plot the confidence band\n",
        "        arr_err *= 1.96\n",
        "        ax.fill_between(range(arr.shape[1]), arr.mean(axis=0) - arr_err, arr.mean(axis=0) + arr_err, alpha=0.3,\n",
        "                        color=color)\n",
        "        # save the plot handle\n",
        "        h_list.append(h)\n",
        "\n",
        "    # plot legends\n",
        "    ax.set_title(f\"{fig_title}\")\n",
        "    ax.legend(handles=h_list)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhFS07Xxtd2g"
      },
      "source": [
        "## Define a shedule for epsilon-greedy policy\n",
        "\n",
        "Here, we define a shedule function to return the epsilon for each time step t. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wv1Jg0nPtd2g"
      },
      "outputs": [],
      "source": [
        "#Currently Linear decay rate used.\n",
        "\n",
        "# class LinearSchedule(object):\n",
        "#     \"\"\" This schedule returns the value linearly\"\"\"\n",
        "#     def __init__(self, start_value, end_value, duration):\n",
        "#         # start value\n",
        "#         self._start_value = start_value\n",
        "#         # end value\n",
        "#         self._end_value = end_value\n",
        "#         # time steps that value changes from the start value to the end value\n",
        "#         self._duration = duration\n",
        "#         # difference between the start value and the end value\n",
        "#         self._schedule_amount = start_value - end_value\n",
        "\n",
        "#     def get_value(self, time):\n",
        "#         # logic: if time > duration, use the end value, else use the scheduled value\n",
        "#         \"\"\" CODE HERE: return the epsilon for each time step within the duration.\n",
        "#         \"\"\"\n",
        "#         if time > self._duration:\n",
        "#             return self._end_value\n",
        "#         else:\n",
        "#             # base_lr * ((1 - float(iter) / max_iter) ** power)\n",
        "#             power = 1\n",
        "#             self._schedule_amount = self._start_value * ((1 - float(time) / self._duration) ** power)\n",
        "#             return round(self._schedule_amount, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring Atari Pong"
      ],
      "metadata": {
        "id": "HeDzlxO5trPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_env = gym.make(\"BoxingNoFrameskip-v4\")"
      ],
      "metadata": {
        "id": "SaCiE0fhtvu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Action space size: \", test_env.action_space.n)\n",
        "print(\"Action space desc: \", test_env.unwrapped.get_action_meanings())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KY24CWhZwwKR",
        "outputId": "c5d64cb2-4be1-4dbc-ddd9-b9761872ca56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action space size:  18\n",
            "Action space desc:  ['NOOP', 'FIRE', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'UPRIGHT', 'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT', 'UPFIRE', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Display resolution: \", test_env.observation_space.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_HgC0cO3Yoo",
        "outputId": "76b09b82-d2a1-45f9-91d7-61bd22953138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Display resolution:  (210, 160, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network"
      ],
      "metadata": {
        "id": "MbGgfmPSMI99"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxfrlUdgtd2g"
      },
      "source": [
        "## Define the DQN agent"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "e_b_ZXOhPprH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env, noop_max=30):\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
        "        No-op is assumed to be action 0.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.noop_max = noop_max\n",
        "        self.override_num_noops = None\n",
        "        self.noop_action = 0\n",
        "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
        "        self.env.reset(**kwargs)\n",
        "        if self.override_num_noops is not None:\n",
        "            noops = self.override_num_noops\n",
        "        else:\n",
        "            noops = self.unwrapped.np_random.integers(1, self.noop_max + 1) #pylint: disable=E1101\n",
        "        assert noops > 0\n",
        "        obs = None\n",
        "        for _ in range(noops):\n",
        "            obs, _, done, _ = self.env.step(self.noop_action)\n",
        "            if done:\n",
        "                obs = self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(FireResetEnv, self).__init__(env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "\n",
        "    def reset(self):\n",
        "        self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        return obs\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        super(MaxAndSkipEnv, self).__init__(env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = collections.deque(maxlen=2)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            self._obs_buffer.append(obs)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self): \n",
        "        #or NoopResetEnv\n",
        "        # noops = (self.env.unwrapped.np_random.integers(1, 31)\n",
        "        #     if self.noop_max > 0\n",
        "        #     else 0)\n",
        "        # for _ in range(noops):\n",
        "        #     _, _, terminated, truncated, step_info = self.env.step(0)\n",
        "        #     reset_info.update(step_info) #better to Global time step update, maybe doesn't matter that much\n",
        "        #     if terminated or truncated:\n",
        "        #         _, reset_info = self.env.reset()\n",
        "\n",
        "        self._obs_buffer.clear()\n",
        "        obs = self.env.reset()\n",
        "        self._obs_buffer.append(obs)\n",
        "        return obs\n",
        "\n",
        "\n",
        "class ProcessFrame84(gym.ObservationWrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(ProcessFrame84, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return ProcessFrame84.process(obs)\n",
        "\n",
        "    @staticmethod\n",
        "    def process(frame):\n",
        "        if frame.size == 210 * 160 * 3:\n",
        "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
        "        elif frame.size == 250 * 160 * 3:\n",
        "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
        "        else:\n",
        "            assert False, \"Unknown resolution.\"\n",
        "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
        "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
        "        x_t = resized_screen[18:102, :]\n",
        "        x_t = np.reshape(x_t, [84, 84, 1])\n",
        "        return x_t.astype(np.uint8)\n",
        "\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env, n_steps, dtype=np.float32):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        self.dtype = dtype\n",
        "        old_space = env.observation_space\n",
        "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
        "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
        "\n",
        "    def reset(self):\n",
        "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
        "        return self.observation(self.env.reset())\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.buffer[:-1] = self.buffer[1:]\n",
        "        self.buffer[-1] = observation\n",
        "        return self.buffer\n",
        "\n",
        "\n",
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(ImageToPyTorch, self).__init__(env)\n",
        "        old_shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], \n",
        "                                old_shape[0], old_shape[1]), dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def observation(self, obs):\n",
        "        return np.array(obs).astype(np.float32) / 255.0\n",
        "\n",
        "def make_env(env_name):\n",
        "    env = gym.make(env_name)\n",
        "    env = NoopResetEnv(env)\n",
        "    env = MaxAndSkipEnv(env)\n",
        "    env = FireResetEnv(env)\n",
        "    env = ProcessFrame84(env)\n",
        "    env = ImageToPyTorch(env)\n",
        "    env = BufferWrapper(env, 4)\n",
        "    return ScaledFloatFrame(env)"
      ],
      "metadata": {
        "id": "PRqrbUmFo5Pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7VJ5SK0td2m"
      },
      "outputs": [],
      "source": [
        "####COMMENT THIS CELL TO USE DUELING ARCHITECTURE BELOW\n",
        "####ARCHITECTURE FOR DQN AND DDQN\n",
        "\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "class DeepQNet(nn.Module):\n",
        "    def __init__(self, input_dim, dim_hidden_layer, output_dim):\n",
        "        super(DeepQNet, self).__init__()\n",
        "\n",
        "\n",
        "        # define the input dimension\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        # define the number of the hidden layers\n",
        "        self.hidden_dim = dim_hidden_layer\n",
        "\n",
        "        # define the output dimension\n",
        "        self.output_dim = output_dim\n",
        "    \n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_dim[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        conv_out_size = self._get_conv_out(self.input_dim)\n",
        "        # print(conv_out_size)\n",
        "        # o = self.conv(torch.zeros(1, *self.input_dim))\n",
        "        # conv_out_size = int(np.prod(o.size()))\n",
        "        #Single clip\n",
        "        self.fc = nn.Sequential(\n",
        "                nn.Linear(conv_out_size, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(512, output_dim)\n",
        "            )\n",
        "        \n",
        "    # In order to code a generic model (for all the games) that can accept different input shape, we will use a simple function, _get_conv_out() \n",
        "    # that accepts the input shape and applies the convolution layer to a fake tensor of such a shape\n",
        "    def _get_conv_out(self, shape):\n",
        "        o = self.conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "        return self.fc(conv_out)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ARCHITECTURE FOR DUELING DDQN\n",
        "\n",
        "# import torchvision\n",
        "# from PIL import Image\n",
        "# class DeepQNet(nn.Module):\n",
        "#     def __init__(self, input_dim, dim_hidden_layer, output_dim):\n",
        "#         super(DeepQNet, self).__init__()\n",
        "\n",
        "#         \"\"\"CODE HERE: construct your Deep neural network\n",
        "#         \"\"\"\n",
        "#         # define the input dimension\n",
        "#         self.input_dim = input_dim\n",
        "\n",
        "#         # define the number of the hidden layers\n",
        "#         self.hidden_dim = dim_hidden_layer\n",
        "\n",
        "#         # define the output dimension\n",
        "#         self.output_dim = output_dim\n",
        "    \n",
        "#         self.conv = nn.Sequential(\n",
        "#             nn.Conv2d(input_dim[0], 32, kernel_size=8, stride=4),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "#             nn.ReLU()\n",
        "#         )\n",
        "\n",
        "#         conv_out_size = self._get_conv_out(self.input_dim)\n",
        "#         # print(conv_out_size)\n",
        "#         # o = self.conv(torch.zeros(1, *self.input_dim))\n",
        "#         # conv_out_size = int(np.prod(o.size()))\n",
        "\n",
        "#         self.fc_state = nn.Sequential(\n",
        "#                 nn.Linear(conv_out_size, 512),\n",
        "#                 nn.ReLU(),\n",
        "#                 nn.Linear(512, 1)\n",
        "#             )\n",
        "        \n",
        "#         self.fc_action = nn.Sequential(\n",
        "#                 nn.Linear(conv_out_size, 512),\n",
        "#                 nn.ReLU(),\n",
        "#                 nn.Linear(512, output_dim)\n",
        "#             )\n",
        "\n",
        "#     #For single clip DDQN, change the final dimension to 1024 for proper comparison with DDDQN    \n",
        "        \n",
        "#     # In order to code a generic model (for all the games) that can accept different input shape, we will use a simple function, _get_conv_out() \n",
        "#     # that accepts the input shape and applies the convolution layer to a fake tensor of such a shape\n",
        "#     def _get_conv_out(self, shape):\n",
        "#         o = self.conv(torch.zeros(1, *shape))\n",
        "#         return int(np.prod(o.size()))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "        \n",
        "#         fstate = self.fc_state(conv_out)\n",
        "#         faction = self.fc_action(conv_out)\n",
        "\n",
        "#         actn = faction - faction.mean(dim=-1, keepdim=True)\n",
        "#         # $Q(s, a) =V(s) + \\Big(A(s, a) - \\frac{1}{|\\mathcal{A}|} \\sum_{a' \\in \\mathcal{A}} A(s, a')\\Big)$\n",
        "#         q = fstate + actn\n",
        "        \n",
        "#         return q\n"
      ],
      "metadata": {
        "id": "uPWwl4UKKRg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DeepQNet((4, 84, 84), 32, 6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItDl_98JIw0p",
        "outputId": "6ceaa5d1-1003-4668-b262-d6d8d6505d69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeepQNet(\n",
              "  (conv): Sequential(\n",
              "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
              "    (3): ReLU()\n",
              "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (5): ReLU()\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent(object):\n",
        "    # initialize the agent\n",
        "    def __init__(self,\n",
        "                 params,\n",
        "                 ):\n",
        "        # save the parameters\n",
        "        self.params = params\n",
        "\n",
        "        # environment parameters\n",
        "        self.action_dim = params['action_dim']\n",
        "        self.obs_dim = params['observation_dim']\n",
        "\n",
        "        # executable actions\n",
        "        self.action_space = params['action_space']\n",
        "\n",
        "        # create behavior policy network\n",
        "        self.behavior_policy_net = DeepQNet(input_dim=params['observation_dim'],\n",
        "                                            dim_hidden_layer=params['hidden_layer_dim'],\n",
        "                                            output_dim=params['action_dim'])\n",
        "        # create target network\n",
        "        self.target_policy_net = DeepQNet(input_dim=params['observation_dim'],\n",
        "                                          dim_hidden_layer=params['hidden_layer_dim'],\n",
        "                                          output_dim=params['action_dim'])\n",
        "\n",
        "        # initialize target network with behavior network\n",
        "        self.behavior_policy_net.apply(customized_weights_init)\n",
        "        self.target_policy_net.load_state_dict(self.behavior_policy_net.state_dict())\n",
        "\n",
        "        # send the agent to a specific device: cpu or gpu\n",
        "        self.device = torch.device(\"cuda\")\n",
        "        self.behavior_policy_net.to(self.device)\n",
        "        self.target_policy_net.to(self.device)\n",
        "\n",
        "        # optimizer\n",
        "        self.optimizer = torch.optim.Adam(self.behavior_policy_net.parameters(), lr=params['learning_rate'])\n",
        "        \n",
        "        # self.loss_fn = nn.MSELoss(reduction='mean')\n",
        "        self.loss_fn = nn.SmoothL1Loss(reduction='mean')\n",
        "\n",
        "\n",
        "    # get action\n",
        "    def get_action(self, obs, eps):\n",
        "        if np.random.random() < eps:  # with probability eps, the agent selects a random action\n",
        "            action = self.action_space.sample()\n",
        "        else:  # with probability 1 - eps, the agent selects a greedy policy\n",
        "            obs = self._arr_to_tensor(obs) #.view(1, -1)\n",
        "            with torch.no_grad():\n",
        "                q_values = self.behavior_policy_net(obs)\n",
        "                action = q_values.max(dim=1)[1].item()\n",
        "        return action\n",
        "\n",
        "    # update behavior policy\n",
        "    def update_behavior_policy(self, batch_data):\n",
        "        # convert batch data to tensor and put them on device\n",
        "        batch_data_tensor = self._batch_to_tensor(batch_data)\n",
        "\n",
        "        # get the transition data\n",
        "        obs_tensor = batch_data_tensor['obs']\n",
        "        actions_tensor = batch_data_tensor['action']\n",
        "        next_obs_tensor = batch_data_tensor['next_obs']\n",
        "        rewards_tensor = batch_data_tensor['reward']\n",
        "        dones_tensor = batch_data_tensor['done']\n",
        "\n",
        "       \n",
        "        #DOUBLE Q-LEARNING BEHAVIOR NETWORK UPDATE\n",
        "\n",
        "        # q_val_b = self.behavior_policy_net(obs_tensor)\n",
        "        \n",
        "        # with torch.no_grad():\n",
        "        #   q_val_t = self.target_policy_net(next_obs_tensor)\n",
        "        #   target_q_val = q_val_b.clone().detach()\n",
        "\n",
        "        #   actions_tensor = actions_tensor.reshape((actions_tensor.shape[0],))\n",
        "          \n",
        "        #   q_val_bn = self.behavior_policy_net(next_obs_tensor)\n",
        "          \n",
        "        #   actn = torch.argmax(q_val_bn,dim=1)\n",
        "        #   indices = np.arange(actions_tensor.shape[0])\n",
        "        #   q_next_val = (q_val_t[indices, actn]).view(-1,1)\n",
        "\n",
        "        #   # compute the TD target using the target network\n",
        "\n",
        "        #   target_q_val[indices, actions_tensor] = (rewards_tensor + self.params['gamma'] *(1 - dones_tensor) * \n",
        "        #                                     (q_next_val).detach().view(-1, 1)).reshape((actions_tensor.shape[0],))\n",
        "        # # compute the loss\n",
        "        # td_loss = self.loss_fn(q_val_b,target_q_val)\n",
        "        \n",
        "        #loss = torch.mean(weights * losses)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #DQN BEHAVIOR NETWORK UPDATE....COMMMENT FOR USING DDQN OR DUELING DDQN\n",
        "\n",
        "        # compute the q value estimation using the behavior network\n",
        "        Q_behaviour = self.behavior_policy_net(obs_tensor).gather(1, actions_tensor)\n",
        "        \n",
        "        '''gather will index the rows of the q-values (i.e. the per-sample q-values in a batch of q-values) \n",
        "        by the batch-list of actions. The result will be the same as if you had done the following \n",
        "        (though it will be much faster than a loop):'''\n",
        "\n",
        "        with torch.no_grad():\n",
        "          # compute the TD target using the target network\n",
        "          Q_target_next = self.target_policy_net(next_obs_tensor).detach().max(1)[0].unsqueeze(1)\n",
        "          Q_targets = rewards_tensor + (self.params[\"gamma\"] * Q_target_next * (1 - dones_tensor))\n",
        "        \n",
        "        # compute the loss\n",
        "        td_loss = self.loss_fn(Q_behaviour, Q_targets) # me\n",
        "\n",
        "        # minimize the loss\n",
        "        self.optimizer.zero_grad()\n",
        "        td_loss.backward()\n",
        "\n",
        "        #UNCOMMENT FOR DUELING DDQN  \n",
        "        # torch.nn.utils.clip_grad_norm_(self.behavior_policy_net.parameters(), max_norm=10) \n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return td_loss.item()\n",
        "\n",
        "    # update update target policy\n",
        "    def update_target_policy(self):\n",
        "        # hard update\n",
        "        \"\"\"CODE HERE: \n",
        "                Copy the behavior policy network to the target network\n",
        "        \"\"\"\n",
        "        self.target_policy_net.load_state_dict(self.behavior_policy_net.state_dict())\n",
        "\n",
        "    # load trained model\n",
        "    def load_model(self, model_file):\n",
        "        # load the trained model\n",
        "        self.behavior_policy_net.load_state_dict(torch.load(model_file, map_location=self.device))\n",
        "        self.behavior_policy_net.eval()\n",
        "        self.target_policy_net.load_state_dict(self.behavior_policy_net.state_dict())\n",
        "        print(\"here\")\n",
        "\n",
        "    # auxiliary functions\n",
        "    def _arr_to_tensor(self, arr):\n",
        "        # arr = np.array(arr)\n",
        "        # arr_tensor = torch.from_numpy(arr).float().to(self.device)\n",
        "\n",
        "        arr = np.array([arr], copy=False)\n",
        "        arr_tensor = torch.tensor(arr).to(self.device)\n",
        "        return arr_tensor\n",
        "\n",
        "    def _batch_to_tensor(self, batch_data):\n",
        "        # store the tensor\n",
        "        batch_data_tensor = {'obs': [], 'action': [], 'reward': [], 'next_obs': [], 'done': []}\n",
        "        # get the numpy arrays\n",
        "        obs_arr, action_arr, reward_arr, next_obs_arr, done_arr = batch_data\n",
        "        # convert to tensors\n",
        "        batch_data_tensor['obs'] = torch.tensor(obs_arr, dtype=torch.float32).to(self.device)\n",
        "        batch_data_tensor['action'] = torch.tensor(action_arr).long().view(-1, 1).to(self.device)\n",
        "        batch_data_tensor['reward'] = torch.tensor(reward_arr, dtype=torch.float32).view(-1, 1).to(self.device)\n",
        "        batch_data_tensor['next_obs'] = torch.tensor(next_obs_arr, dtype=torch.float32).to(self.device)\n",
        "        batch_data_tensor['done'] = torch.tensor(done_arr, dtype=torch.float32).view(-1, 1).to(self.device)\n",
        "\n",
        "        return batch_data_tensor"
      ],
      "metadata": {
        "id": "QOwLm082MHkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HMiWiwy9Vzv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMDPZuNBtd2g"
      },
      "source": [
        "## Define the training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9qDstbltd2h"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import pickle\n",
        "\n",
        "import gym.spaces, cv2\n",
        "\n",
        "def train_dqn_agent(env, params):\n",
        "    # create the DQN agent\n",
        "    my_agent = DQNAgent(params)\n",
        "\n",
        "    # my_agent.load_model(\"KrullNoFrameskip-v4_duel.pt\")\n",
        "\n",
        "\n",
        "    # create the epsilon-greedy schedule\n",
        "    # my_schedule = LinearSchedule(start_value=params['epsilon_start_value'],\n",
        "    #                              end_value=params['epsilon_end_value'],\n",
        "    #                              duration=params['epsilon_duration'])\n",
        "    \n",
        "    eps_t = 1.0\n",
        "\n",
        "    # create the replay buffer\n",
        "    replay_buffer = ReplayBuffer(params['replay_buffer_size'])\n",
        "\n",
        "    # training variables\n",
        "    episode_t = 0\n",
        "    rewards = []\n",
        "    train_returns = []\n",
        "    train_loss = []\n",
        "    loss = 0\n",
        "\n",
        "    # reset the environment\n",
        "    obs = env.reset()\n",
        "\n",
        "    # start training\n",
        "    pbar = tqdm.trange(params['total_training_time_step'])\n",
        "    last_best_return = 0\n",
        "    for t in pbar:\n",
        "        # scheduled epsilon at time step t\n",
        "        if eps_t > 0.01:\n",
        "          eps_t -= 0.255e-6 #1.15e-6 more exploitation but then again....0.43 for boxing\n",
        "\n",
        "        # get one epsilon-greedy action\n",
        "        action = my_agent.get_action(obs, eps_t)\n",
        "\n",
        "        # step in the environment\n",
        "        next_obs, reward, done, _ = env.step(action)\n",
        "\n",
        "        # add to the buffer\n",
        "        replay_buffer.add(obs, action, reward, next_obs, done)\n",
        "        rewards.append(reward)\n",
        "            \n",
        "        \n",
        "        # if t%1000_000==0 and t!=0:\n",
        "        #     train_lists = [train_returns, train_loss]\n",
        "        #     print(len(train_loss))\n",
        "        #     print(len(train_returns))\n",
        "        #     with open('RL_DQN_data_krull_duel_1M_data', 'wb') as file:\n",
        "        #         pickle.dump(train_lists, file)\n",
        "        #     files.download('RL_DQN_data_krull_duel_1M_data')    \n",
        "        #     files.download('KrullNoFrameskip-v4_duel.pt')    \n",
        "\n",
        "\n",
        "\n",
        "        # if t%3499_999==0 and t!=0:\n",
        "        #     train_lists = [train_returns, train_loss]\n",
        "        #     print(len(train_loss))\n",
        "        #     print(len(train_returns))\n",
        "        #     with open('RL_DQN_data_krull_duel_3.5M_data', 'wb') as file:\n",
        "        #         pickle.dump(train_lists, file)\n",
        "        #     files.download('RL_DQN_data_krull_duel_3.5M_data')    \n",
        "        #     files.download('KrullNoFrameskip-v4_duel.pt')    \n",
        "\n",
        "\n",
        "\n",
        "        # check termination\n",
        "        if done or t == params['max_time_step_per_episode'] - 1:\n",
        "            # compute the return\n",
        "            G = 0\n",
        "            for r in reversed(rewards):\n",
        "                G = r + G\n",
        "\n",
        "            if G > last_best_return:\n",
        "                torch.save(my_agent.behavior_policy_net.state_dict(), f\"./{params['model_name']}\")\n",
        "                last_best_return = G\n",
        "\n",
        "\n",
        "\n",
        "            # store the return\n",
        "            train_returns.append(G)\n",
        "            episode_idx = len(train_returns)\n",
        "\n",
        "            # print the information\n",
        "            pbar.set_description(\n",
        "                f\"Ep={episode_idx} | \"\n",
        "                f\"G={np.mean(train_returns[-10:]) if train_returns else 0:.2f} | \"\n",
        "                f\"Eps={eps_t}\"\n",
        "            )\n",
        "\n",
        "            # reset the environment\n",
        "            episode_t, rewards = 0, []\n",
        "            obs = env.reset()\n",
        "\n",
        "        else:\n",
        "            # increment\n",
        "            obs = next_obs\n",
        "            episode_t += 1\n",
        "\n",
        "        if t > params['start_training_step']:\n",
        "            # update the behavior model\n",
        "            if not np.mod(t, params['freq_update_behavior_policy']):\n",
        "                \"\"\" CODE HERE:\n",
        "                    Update the behavior policy network\n",
        "                \"\"\"\n",
        "                batch = replay_buffer.sample_batch(params[\"batch_size\"])\n",
        "                loss = my_agent.update_behavior_policy(batch)\n",
        "                \n",
        "                # store the loss\n",
        "                train_loss.append(loss)\n",
        "\n",
        "            # update the target model\n",
        "            if not np.mod(t, params['freq_update_target_policy']):\n",
        "                \"\"\" CODE HERE:\n",
        "                    Update the behavior policy network\n",
        "                \"\"\"\n",
        "                my_agent.update_target_policy()\n",
        "\n",
        "    # save the results\n",
        "    return train_returns, train_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "episode_t = 0\n",
        "if episode_t%1000_000==0:\n",
        "  print(\"hello\")"
      ],
      "metadata": {
        "id": "ZWXzgiyId7u9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94b42983-de57-4b04-a6f5-30da0407a5d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Almost DQN\n",
        "# def update_behavior_policy(self, batch_data):\n",
        "#     # convert batch data to tensor and put them on device\n",
        "#     batch_data_tensor = self._batch_to_tensor(batch_data)\n",
        "\n",
        "#     # get the transition data\n",
        "#     obs_tensor = batch_data_tensor['obs']\n",
        "#     actions_tensor = batch_data_tensor['action']\n",
        "#     next_obs_tensor = batch_data_tensor['next_obs']\n",
        "#     rewards_tensor = batch_data_tensor['reward']\n",
        "#     dones_tensor = batch_data_tensor['done']\n",
        "\n",
        "#     \"\"\"CODE HERE:\n",
        "#             Compute the predicted Q values using the behavior policy network\n",
        "#     \"\"\"\n",
        "#     # compute the q value estimation using the behavior network\n",
        "#     Q_behaviour = self.behavior_policy_net(obs_tensor).gather(1, actions_tensor)\n",
        "    \n",
        "#     '''gather will index the rows of the q-values (i.e. the per-sample q-values in a batch of q-values) \n",
        "#     by the batch-list of actions. The result will be the same as if you had done the following \n",
        "#     (though it will be much faster than a loop):'''\n",
        "    \n",
        "#     # compute the TD target using the target network\n",
        "#     Q_target_next = self.target_policy_net(next_obs_tensor).detach().max(1)[0].unsqueeze(1)\n",
        "#     Q_targets = rewards_tensor + (self.params[\"gamma\"] * Q_target_next * (1 - dones_tensor))\n",
        "    \n",
        "#     # compute the loss\n",
        "#     td_loss = self.loss_fn(Q_behaviour, Q_targets) # me\n",
        "\n",
        "#     # minimize the loss\n",
        "#     self.optimizer.zero_grad()\n",
        "#     td_loss.backward()\n",
        "#     self.optimizer.step()\n",
        "\n",
        "#     return td_loss.item()\n"
      ],
      "metadata": {
        "id": "YYlBH0Pc_Vve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e1zZe2_td2n"
      },
      "outputs": [],
      "source": [
        "# if __name__ == '__main__':\n",
        "#     # set the random seed\n",
        "#     np.random.seed(1234)\n",
        "#     random.seed(1234)\n",
        "#     torch.manual_seed(1234)\n",
        "\n",
        "#     # create environment\n",
        "#     my_env = make_env(\"PongNoFrameskip-v4\")\n",
        "#     # create training parameters\n",
        "    # train_parameters = {\n",
        "    #     'observation_dim': my_env.observation_space.shape,\n",
        "    #     'action_dim': my_env.action_space.n,\n",
        "    #     'action_space': my_env.action_space,\n",
        "    #     'hidden_layer_dim': 32,\n",
        "    #     'gamma': 0.999,\n",
        "        \n",
        "    #     'max_time_step_per_episode': 5000, #not compulsory, not used\n",
        "\n",
        "    #     'total_training_time_step': 1000000,\n",
        "\n",
        "    #     'epsilon_start_value': 1.0,\n",
        "    #     'epsilon_end_value': 0.01,\n",
        "    #     'epsilon_duration': 250_000,\n",
        "\n",
        "    #     'replay_buffer_size': 50000, #before 25k\n",
        "    #     'start_training_step': 2500, # before 2k\n",
        "    #     'freq_update_behavior_policy': 4,\n",
        "    #     'freq_update_target_policy': 3000, #before 10k\n",
        "\n",
        "    #     'batch_size': 32,\n",
        "    #     'learning_rate': 3e-4, #3e-4 for pong\n",
        "\n",
        "#         'model_name': \"PongNoFrameskip_duel.pt\"\n",
        "#     }\n",
        "\n",
        "\n",
        "\n",
        "    # create experiment\n",
        "    # train_returns, train_loss = train_dqn_agent(my_env, train_parameters)\n",
        "\n",
        "\n",
        "# agent = DuelingDDQNAgent(gamma=0.99, epsilon=1.0, lr=0.0001,\n",
        "#                     input_dims=(env.observation_space.shape),\n",
        "#                     n_actions=env.action_space.n, mem_size=50000, eps_min=0.1,\n",
        "#                     batch_size=32, replace=10000, eps_dec=1e-5,\n",
        "#                     chkpt_dir='models/', algo='DuelingDDQNAgent',\n",
        "#                     env_name='PongNoFrameskip-v4')\n",
        "\n",
        "\n",
        "# configs = {\n",
        "#     # Number of updates\n",
        "#     'updates': 1_000_000,\n",
        "#     # Number of epochs to train the model with sampled data.\n",
        "#     'epochs': 8,\n",
        "#     # Number of worker processes\n",
        "#     'n_workers': 8,\n",
        "#     # Number of steps to run on each process for a single update\n",
        "#     'worker_steps': 4,\n",
        "#     # Mini batch size\n",
        "#     'mini_batch_size': 32,\n",
        "#     # Target model updating interval\n",
        "#     'update_target_model': 250,\n",
        "#     # Learning rate.\n",
        "#     'learning_rate': FloatDynamicHyperParam(1e-4, (0, 1e-3)),\n",
        "# }\n",
        "\n",
        "\n",
        "# DQN_HYPERPARAMS = {\n",
        "# \t'dueling': False,\n",
        "# \t'noisy_net': False,\n",
        "# \t'double_DQN': False,\n",
        "# \t'n_multi_step': 2,\n",
        "# \t'buffer_start_size': 10001,\n",
        "# \t'buffer_capacity': 15000,\n",
        "# \t'epsilon_start': 1.0,\n",
        "# \t'epsilon_decay': 10**5,\n",
        "# \t'epsilon_final': 0.02,\n",
        "# \t'learning_rate': 5e-5,\n",
        "# \t'gamma': 0.99,\n",
        "# \t'n_iter_update_target': 1000\n",
        "# }\n",
        "\n",
        "\n",
        "# self,\n",
        "# n_actions,\n",
        "# n_features,\n",
        "# learning_rate=0.001,\n",
        "# reward_decay=0.9,\n",
        "# e_greedy=0.9,\n",
        "# replace_target_iter=200,\n",
        "# memory_size=500,\n",
        "# batch_size=32,\n",
        "# e_greedy_increment=None,\n",
        "# output_graph=False,\n",
        "# dueling=True,\n",
        "# sess=None,\n",
        "\n",
        "\n",
        "\n",
        "# RANDOM_SEED = 0\n",
        "# torch.manual_seed(RANDOM_SEED)\n",
        "# env.seed(RANDOM_SEED)\n",
        "# np.random.seed(RANDOM_SEED)\n",
        "# random.seed(RANDOM_SEED)\n",
        "\n",
        "\n",
        "#Reeward clipping.......\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # set the random seed\n",
        "    np.random.seed(1234)\n",
        "    random.seed(1234)\n",
        "    torch.manual_seed(1234)\n",
        "\n",
        "    # create environment\n",
        "    # my_env = make_env(\"PongDeterministic-v4\")\n",
        "    my_env = make_env(\"KrullNoFrameskip-v4\")\n",
        "    # create training parameters\n",
        "    train_parameters = {\n",
        "        'observation_dim': my_env.observation_space.shape,\n",
        "        'action_dim': my_env.action_space.n,\n",
        "        'action_space': my_env.action_space,\n",
        "        'hidden_layer_dim': 32,\n",
        "        'gamma': 0.99, \n",
        "        \n",
        "        'max_time_step_per_episode': 25000, \n",
        "\n",
        "        'total_training_time_step': 5000_000,\n",
        "\n",
        "        'epsilon_start_value': 1.0,\n",
        "        'epsilon_end_value': 0.01,\n",
        "        'epsilon_duration': 250_000,\n",
        "\n",
        "        'replay_buffer_size': 50000, \n",
        "        'start_training_step': 5000,\n",
        "        'freq_update_behavior_policy': 4,\n",
        "        'freq_update_target_policy': 10_000, \n",
        "\n",
        "        'batch_size': 32,\n",
        "        'learning_rate': 2.25e-4,\n",
        "        'model_name': \"KrullNoFrameskip-v4_dqn_5M.pt\"\n",
        "    }\n",
        "\n",
        "    # create experiment\n",
        "    train_returns, train_loss = train_dqn_agent(my_env, train_parameters)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_lists = [train_returns, train_loss]\n",
        "print(len(train_loss))\n",
        "print(len(train_returns))\n",
        "with open('RL_DQN_data_krull_dqn_5M1_data', 'wb') as file:\n",
        "    pickle.dump(train_lists, file)\n",
        "files.download('RL_DQN_data_krull_dqn_5M1_data')    \n",
        "files.download('KrullNoFrameskip-v4_dqn_5M.pt')    \n"
      ],
      "metadata": {
        "id": "XupbT3C522I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_curves([np.array([train_returns])], ['Krull dqn'], ['r'], 'Score', 'Atari Krull score wrt episode')"
      ],
      "metadata": {
        "id": "u35aQuAZ6MrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_curves([np.array([train_returns])], ['dueling ddqn'], ['r'], 'Score', 'Atari Pong score wrt episode')"
      ],
      "metadata": {
        "id": "gcozJCfTMXHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_curves([np.array([train_returns])], ['dqn'], ['r'], 'discounted return', 'Atari Pong discounted returns wrt episode')"
      ],
      "metadata": {
        "id": "aqSO7A5Cl7B3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_curves([np.array([train_returns])], ['dqn'], ['r'], 'discounted return', 'Atari Pong discounted returns wrt episode')"
      ],
      "metadata": {
        "id": "JIjSLptDbmc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_curves([np.array([train_returns])], ['dueling ddqn'], ['r'], 'Score', 'Atari Seaquest score wrt episode')"
      ],
      "metadata": {
        "id": "eqY-XClMIfsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statistics import mean\n",
        "\n",
        "my_env = make_env(\"PongNoFrameskip-v4\")\n",
        "my_agent = DQNAgent(train_parameters)\n",
        "my_agent.load_model(\"PongNoFrameskip.pt\")\n",
        "\n",
        "e_sum=[]\n",
        "\n",
        "np.random.seed(1234)\n",
        "random.seed(1234)\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "\n",
        "for episode in range(100):\n",
        "    done = False\n",
        "    sum_reward=0\n",
        "    i=0\n",
        "    obs = my_env.reset()\n",
        "    while not done and i < train_parameters['max_time_step_per_episode']:\n",
        "      action = my_agent.get_action(obs, 0.05)\n",
        "      next_obs, reward, done,_ = my_env.step(action)\n",
        "      sum_reward += reward\n",
        "      obs = next_obs\n",
        "      i += 1\n",
        "    e_sum.append(sum_reward) \n",
        "    print()\n",
        "    print(sum_reward)\n",
        "    print()\n",
        "print(sum(e_sum)/5)    "
      ],
      "metadata": {
        "id": "4pt8fvr46uOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_agent.load_model(\"PongNoFrameskip.pt\")\n",
        "print"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2zT3J-h-9QT",
        "outputId": "763f642b-2a3f-4d77-ac20-9784c274ce14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pickle\n",
        "\n",
        "train_lists = [train_returns, train_loss]\n",
        "\n",
        "with open('RL_DQN_data_freeway_ddqn_1M', 'wb') as file:\n",
        "    pickle.dump(train_lists, file)\n",
        "\n",
        "files.download('RL_DQN_data_freeway_ddqn_1M')    "
      ],
      "metadata": {
        "id": "lsRahCmXkILJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "2519d647-f430-4388-b6dc-d89fb969eb03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_149d26ef-85fa-4467-a90a-0d838eb8f5e8\", \"triallalsdk\", 2243968)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pickle\n",
        "\n",
        "with open('RL_DQN_data_freeway_ddqn_1M', 'rb') as file:\n",
        "    dd = pickle.load(file)"
      ],
      "metadata": {
        "id": "fZjRnnkY50kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dd[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxw1qvDE6zEJ",
        "outputId": "6cfc7214-24a2-4db7-e8e2-04e080da63fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "489"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B54ceG3JICZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F7cDIYnVICbv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}